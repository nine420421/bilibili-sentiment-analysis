import streamlit as st
import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns
from wordcloud import WordCloud
import jieba
import plotly.express as px
import plotly.graph_objects as go
from collections import Counter
import base64
import io
import os

# è®¾ç½®é¡µé¢
st.set_page_config(
    page_title="Bç«™è¯„è®ºæƒ…æ„Ÿåˆ†æç³»ç»Ÿ",
    page_icon="ğŸ”",
    layout="wide"
)

# ä¿®æ­£å­—ä½“è®¾ç½®
try:
    plt.rcParams['font.sans-serif'] = ['SimHei', 'DejaVu Sans']
    plt.rcParams['axes.unicode_minus'] = False
except:
    pass  # å¦‚æœå­—ä½“è®¾ç½®å¤±è´¥ï¼Œä½¿ç”¨é»˜è®¤å­—ä½“

def main():
    st.title("ğŸ” Bç«™è§†é¢‘è¯„è®ºæƒ…æ„Ÿåˆ†æç³»ç»Ÿ")
    st.markdown("---")
    st.info("ğŸ“Š ä¸Šä¼ ä½ çš„Bç«™è¯„è®ºæ•°æ®CSVæ–‡ä»¶ï¼Œç³»ç»Ÿå°†è‡ªåŠ¨è¿›è¡Œæƒ…æ„Ÿåˆ†æå¹¶ç”Ÿæˆå¯è§†åŒ–æŠ¥å‘Šã€‚")

    # ç¤ºä¾‹æ•°æ®
    sample_data = {
        'user_name': ['ç”¨æˆ·A', 'ç”¨æˆ·B', 'ç”¨æˆ·C', 'ç”¨æˆ·D'],
        'content_cleaned': ['è¿™ä¸ªè§†é¢‘å¾ˆå¥½çœ‹', 'å†…å®¹ä¸€èˆ¬èˆ¬', 'å¤ªæ£’äº†ï¼Œæ¨è', 'ä¸å–œæ¬¢è¿™ä¸ª'],
        'sentiment_label': ['ç§¯æ', 'æ¶ˆæ', 'ç§¯æ', 'æ¶ˆæ'],
        'sentiment_score': [0.85, 0.25, 0.90, 0.15],
        'like_count': [156, 23, 289, 12],
        'post_time': ['2024-01-01', '2024-01-02', '2024-01-03', '2024-01-04']
    }

    st.sidebar.header("ğŸ“¤ æ•°æ®ä¸Šä¼ ")
    uploaded_file = st.sidebar.file_uploader("é€‰æ‹©CSVæ–‡ä»¶", type=['csv'])

    if uploaded_file is not None:
        try:
            df = pd.read_csv(uploaded_file)
            st.success(f"âœ… æˆåŠŸåŠ è½½ {len(df)} æ¡è¯„è®ºæ•°æ®")
        except Exception as e:
            st.error(f"âŒ æ–‡ä»¶è¯»å–é”™è¯¯: {e}")
            st.warning("âš ï¸ ä½¿ç”¨ç¤ºä¾‹æ•°æ®è¿›è¡Œæ¼”ç¤º")
            df = pd.DataFrame(sample_data)
    else:
        df = pd.DataFrame(sample_data)
        st.warning("âš ï¸ å½“å‰ä½¿ç”¨ç¤ºä¾‹æ•°æ®ï¼Œè¯·ä¸Šä¼ CSVæ–‡ä»¶")

    # æ•°æ®æ¦‚è§ˆ
    st.header("ğŸ“ˆ æ•°æ®æ¦‚è§ˆ")
    col1, col2, col3, col4 = st.columns(4)

    with col1:
        st.metric("æ€»è¯„è®ºæ•°", len(df))
    with col2:
        positive_count = len(df[df['sentiment_label'] == 'ç§¯æ'])
        st.metric("ç§¯æè¯„è®º", positive_count)
    with col3:
        negative_count = len(df[df['sentiment_label'] == 'æ¶ˆæ'])
        st.metric("æ¶ˆæè¯„è®º", negative_count)
    with col4:
        total_likes = df['like_count'].sum()
        st.metric("æ€»ç‚¹èµæ•°", f"{total_likes}")

    # æƒ…æ„Ÿåˆ†æå›¾è¡¨
    st.header("ğŸ“Š æƒ…æ„Ÿåˆ†æ")
    col1, col2 = st.columns(2)

    with col1:
        sentiment_counts = df['sentiment_label'].value_counts()
        fig_pie = px.pie(
            values=sentiment_counts.values,
            names=sentiment_counts.index,
            title='è¯„è®ºæƒ…æ„Ÿåˆ†å¸ƒ',
            color=sentiment_counts.index,
            color_discrete_map={'ç§¯æ': '#2E86AB', 'æ¶ˆæ': '#A23B72'}
        )
        st.plotly_chart(fig_pie, use_container_width=True)

    with col2:
        if 'sentiment_score' in df.columns:
            fig_hist = px.histogram(
                df, x='sentiment_score',
                title='æƒ…æ„Ÿå¾—åˆ†åˆ†å¸ƒ',
                nbins=20,
                color_discrete_sequence=['#2E86AB']
            )
            fig_hist.add_vline(x=0.5, line_dash="dash", line_color="red")
            st.plotly_chart(fig_hist, use_container_width=True)

   # è¯äº‘ç”Ÿæˆ
# è®¾ç½®ä¸­æ–‡å­—ä½“æ”¯æŒ
plt.rcParams['font.sans-serif'] = ['SimHei', 'Microsoft YaHei', 'DejaVu Sans']
plt.rcParams['axes.unicode_minus'] = False

def word_cloud_analysis():
    print("ğŸš€ å¼€å§‹è¯äº‘ä¸é«˜é¢‘è¯åˆ†æ...")
    print("=" * 50)

    # 1. è¯»å–æƒ…æ„Ÿåˆ†æç»“æœ
    print("1. è¯»å–æƒ…æ„Ÿåˆ†ææ•°æ®...")
    df = pd.read_csv('bilibili_comments_with_sentiment.csv')
    print(f"   âœ… åˆ†ææ•°æ®: {len(df)} æ¡è¯„è®º")

    # 2. å‡†å¤‡ä¸åŒæƒ…æ„Ÿçš„è¯é¢‘æ•°æ®
    print("\n2. å‡†å¤‡è¯é¢‘æ•°æ®...")

    def get_words_from_segmented(segmented_str):
        """ä»åˆ†è¯è¯­å¥ä¸­æå–è¯æ±‡åˆ—è¡¨"""
        if isinstance(segmented_str, str):
            # ç§»é™¤æ–¹æ‹¬å·å’Œå¼•å·ï¼Œç„¶ååˆ†å‰²
            words = segmented_str.strip("[]").replace("'", "").split(", ")
            return [word for word in words if len(word) > 1]
        return []

    # è·å–æ‰€æœ‰è¯„è®ºçš„è¯æ±‡
    all_words = []
    for seg_text in df['segmented_words']:
        all_words.extend(get_words_from_segmented(seg_text))

    # æŒ‰æƒ…æ„Ÿåˆ†ç±»è·å–è¯æ±‡
    positive_words = []
    negative_words = []
    neutral_words = []

    for idx, row in df.iterrows():
        words = get_words_from_segmented(row['segmented_words'])
        if row['sentiment_label'] == 'ç§¯æ':
            positive_words.extend(words)
        elif row['sentiment_label'] == 'æ¶ˆæ':
            negative_words.extend(words)
        else:
            neutral_words.extend(words)

    print(f"   æ€»è¯æ±‡é‡: {len(all_words)} ä¸ªè¯")
    print(f"   ç§¯æè¯„è®ºè¯æ±‡: {len(positive_words)} ä¸ª")
    print(f"   æ¶ˆæè¯„è®ºè¯æ±‡: {len(negative_words)} ä¸ª")
    print(f"   ä¸­æ€§è¯„è®ºè¯æ±‡: {len(neutral_words)} ä¸ª")

    # 3. ç”Ÿæˆè¯äº‘
    print("\n3. ç”Ÿæˆè¯äº‘å›¾...")

    # è®¾ç½®ä¸­æ–‡å­—ä½“ï¼ˆé‡è¦ï¼ï¼‰
    font_path = 'C:/Windows/Fonts/simhei.ttf'  # é»‘ä½“å­—ä½“

    # å…¨é‡è¯äº‘
    print("   ç”Ÿæˆå…¨é‡è¯äº‘...")
    all_text = ' '.join(all_words)
    wordcloud_all = WordCloud(
        font_path=font_path,
        width=800,
        height=600,
        background_color='white',
        max_words=100,
        colormap='viridis'
    ).generate(all_text)

    # ç§¯æè¯„è®ºè¯äº‘
    print("   ç”Ÿæˆç§¯æè¯„è®ºè¯äº‘...")
    positive_text = ' '.join(positive_words)
    wordcloud_positive = WordCloud(
        font_path=font_path,
        width=800,
        height=600,
        background_color='white',
        max_words=80,
        colormap='spring'  # æš–è‰²è°ƒ
    ).generate(positive_text)

    # æ¶ˆæè¯„è®ºè¯äº‘
    print("   ç”Ÿæˆæ¶ˆæè¯„è®ºè¯äº‘...")
    negative_text = ' '.join(negative_words)
    wordcloud_negative = WordCloud(
        font_path=font_path,
        width=800,
        height=600,
        background_color='white',
        max_words=80,
        colormap='autumn'  # å†·è‰²è°ƒ
    ).generate(negative_text)

    # 4. ç»˜åˆ¶è¯äº‘å›¾
    print("\n4. ç»˜åˆ¶è¯äº‘å›¾...")
    plt.figure(figsize=(20, 12))

    # å…¨é‡è¯äº‘
    plt.subplot(2, 2, 1)
    plt.imshow(wordcloud_all, interpolation='bilinear')
    plt.title('å…¨é‡è¯„è®ºè¯äº‘å›¾', fontsize=16, fontweight='bold')
    plt.axis('off')

    # ç§¯æè¯„è®ºè¯äº‘
    plt.subplot(2, 2, 2)
    plt.imshow(wordcloud_positive, interpolation='bilinear')
    plt.title('ç§¯æè¯„è®ºè¯äº‘å›¾', fontsize=16, fontweight='bold', color='green')
    plt.axis('off')

    # æ¶ˆæè¯„è®ºè¯äº‘
    plt.subplot(2, 2, 3)
    plt.imshow(wordcloud_negative, interpolation='bilinear')
    plt.title('æ¶ˆæè¯„è®ºè¯äº‘å›¾', fontsize=16, fontweight='bold', color='red')
    plt.axis('off')

    # 5. é«˜é¢‘è¯åˆ†æ
    print("\n5. é«˜é¢‘è¯åˆ†æ...")

    def get_top_words(word_list, top_n=15):
        """è·å–å‰Nä¸ªé«˜é¢‘è¯"""
        word_count = Counter(word_list)
        return word_count.most_common(top_n)

    # è·å–å„ç±»é«˜é¢‘è¯
    top_all = get_top_words(all_words)
    top_positive = get_top_words(positive_words)
    top_negative = get_top_words(negative_words)

    # ç»˜åˆ¶é«˜é¢‘è¯æ¡å½¢å›¾
    plt.subplot(2, 2, 4)

    # å‡†å¤‡æ•°æ® - å–å‰10ä¸ªè¯
    words_all = [word for word, count in top_all[:10]]
    counts_all = [count for word, count in top_all[:10]]

    y_pos = np.arange(len(words_all))
    plt.barh(y_pos, counts_all, color='skyblue')
    plt.yticks(y_pos, words_all, fontproperties='SimHei')
    plt.xlabel('å‡ºç°é¢‘æ¬¡')
    plt.title('å…¨é‡è¯„è®ºé«˜é¢‘è¯TOP10', fontsize=14, fontweight='bold')
    plt.gca().invert_yaxis()  # å€’ç½®Yè½´ï¼Œè®©æœ€é«˜çš„åœ¨é¡¶éƒ¨

    plt.tight_layout()
    plt.savefig('word_cloud_analysis.png', dpi=300, bbox_inches='tight')
    print("   âœ… è¯äº‘å›¾å·²ä¿å­˜: word_cloud_analysis.png")

    # 6. é«˜é¢‘è¯å¯¹æ¯”åˆ†æ
    print("\n6. é«˜é¢‘è¯å¯¹æ¯”åˆ†æ:")

    print(f"\n   ğŸ“Š å…¨é‡è¯„è®ºé«˜é¢‘è¯TOP10:")
    for i, (word, count) in enumerate(top_all[:10], 1):
        print(f"      {i:2d}. {word:8s} : {count:3d} æ¬¡")

    print(f"\n   ğŸ’š ç§¯æè¯„è®ºç‰¹æœ‰é«˜é¢‘è¯:")
    positive_unique = [word for word, count in top_positive if word not in [w for w, c in top_negative[:10]]]
    print(f"      {', '.join(positive_unique[:8])}")

    print(f"\n   ğŸ’” æ¶ˆæè¯„è®ºç‰¹æœ‰é«˜é¢‘è¯:")
    negative_unique = [word for word, count in top_negative if word not in [w for w, c in top_positive[:10]]]
    print(f"      {', '.join(negative_unique[:8])}")

    # 7. æƒ…æ„Ÿè¯æ±‡åˆ†æ
    print(f"\n7. æƒ…æ„Ÿè¯æ±‡æ´å¯Ÿ:")

    # å®šä¹‰ä¸€äº›æƒ…æ„Ÿè¯æ±‡ç¤ºä¾‹
    positive_keywords = ['æ”¯æŒ', 'å‰å®³', 'ç‚¹èµ', 'ä¼˜ç§€', 'å‘å±•', 'è¿›æ­¥', 'å¸Œæœ›', 'æ„Ÿè°¢']
    negative_keywords = ['è´¨ç–‘', 'åå¯¹', 'æµªè´¹', 'é—®é¢˜', 'å›°éš¾', 'æ‹…å¿ƒ', 'å¤±æœ›', 'æ‰¹è¯„']

    pos_count = sum(1 for word in all_words if word in positive_keywords)
    neg_count = sum(1 for word in all_words if word in negative_keywords)

    print(f"   ç§¯ææƒ…æ„Ÿè¯æ±‡å‡ºç°: {pos_count} æ¬¡")
    print(f"   æ¶ˆææƒ…æ„Ÿè¯æ±‡å‡ºç°: {neg_count} æ¬¡")
    print(f"   æƒ…æ„Ÿè¯æ±‡æ¯”ä¾‹: {pos_count / (pos_count + neg_count) * 100:.1f}% ç§¯æ")

    return {
        'all_words': all_words,
        'positive_words': positive_words,
        'negative_words': negative_words,
        'top_all': top_all,
        'top_positive': top_positive,
        'top_negative': top_negative
    }


# è¿è¡Œè¯äº‘åˆ†æ
if __name__ == "__main__":
    word_analysis_results = word_cloud_analysis()

    print("\n" + "=" * 60)
    print("âœ… è¯äº‘å›¾ç”Ÿæˆå®Œæˆ")
    print("âœ… é«˜é¢‘è¯åˆ†æå®Œæˆ")
    print("âœ… æƒ…æ„Ÿè¯æ±‡å¯¹æ¯”å®Œæˆ")
    print("âœ… æ–‡æœ¬æ´å¯ŸæŒ–æ˜å®Œæˆ")
    print("=" * 60)

    # è¯„è®ºè¯¦æƒ…
    st.header("ğŸ’¬ è¯„è®ºè¯¦æƒ…")
    col1, col2 = st.columns(2)

    with col1:
        sentiment_filter = st.multiselect(
            "æƒ…æ„Ÿç­›é€‰",
            options=df['sentiment_label'].unique(),
            default=df['sentiment_label'].unique()
        )

    with col2:
        sort_by = st.selectbox(
            "æ’åºæ–¹å¼",
            ["é»˜è®¤", "ç‚¹èµæ•°", "æƒ…æ„Ÿå¾—åˆ†"]
        )

    filtered_df = df[df['sentiment_label'].isin(sentiment_filter)]

    if sort_by == "ç‚¹èµæ•°" and 'like_count' in filtered_df.columns:
        filtered_df = filtered_df.sort_values(by='like_count', ascending=False)
    elif sort_by == "æƒ…æ„Ÿå¾—åˆ†" and 'sentiment_score' in filtered_df.columns:
        filtered_df = filtered_df.sort_values(by='sentiment_score', ascending=False)

    st.dataframe(
        filtered_df[['user_name', 'content_cleaned', 'sentiment_label', 'sentiment_score', 'like_count']],
        use_container_width=True
    )

    # ä½¿ç”¨è¯´æ˜
    with st.expander("ğŸ“– ä½¿ç”¨è¯´æ˜"):
        st.markdown("""
        ## ä½¿ç”¨æŒ‡å—

        1. **æ•°æ®å‡†å¤‡**
           - ä½¿ç”¨Pythonçˆ¬è™«è·å–Bç«™è¯„è®ºæ•°æ®
           - è¿›è¡Œæ•°æ®æ¸…æ´—å’Œæƒ…æ„Ÿåˆ†æ
           - å¯¼å‡ºä¸ºCSVæ ¼å¼

        2. **ä¸Šä¼ åˆ†æ**
           - åœ¨å·¦ä¾§ä¸Šä¼ CSVæ–‡ä»¶
           - ç³»ç»Ÿè‡ªåŠ¨ç”Ÿæˆåˆ†ææŠ¥å‘Š
           - æŸ¥çœ‹å„ç§å¯è§†åŒ–å›¾è¡¨

        3. **åŠŸèƒ½ç‰¹æ€§**
           - æƒ…æ„Ÿåˆ†å¸ƒåˆ†æ
           - è¯äº‘ç”Ÿæˆ
           - è¯„è®ºè¯¦æƒ…æµè§ˆ

        4. **æŠ€æœ¯æ ˆ**: Python + Streamlit
        """)

if __name__ == "__main__":
    main()




